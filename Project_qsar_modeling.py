# -*- coding: utf-8 -*-
"""Copy of M.Tech_Project_QSAR_Modelling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11jbPvSf0ksq0LFpUTYMqA6qWS5IAwhgv
"""

! wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh
! chmod +x Miniconda3-py37_4.8.2-Linux-x86_64.sh
! bash ./Miniconda3-py37_4.8.2-Linux-x86_64.sh -b -f -p /usr/local
import sys
sys.path.append('/usr/local/lib/python3.7/site-packages/')

! wget https://github.com/raghavagps/Pfeature/raw/master/PyLib/Pfeature.zip

! unzip Pfeature.zip

# Commented out IPython magic to ensure Python compatibility.
# % cd Pfeature

! python setup.py install

! pip python setup.py

! conda install -c bioconda cd-hit -y

import pandas as pd

with open('/content/anticancer_ENNAACT.txt', 'r') as f:
    content = f.readlines()

n = len(content)


label = list('[content[i].strip() for i in range(0,n,2)]')
seq = [content[i].strip() for i in range(1,n,2)]
s1 = pd.Series(label)
s2 = pd.Series(seq)
df1 = pd.DataFrame({'label':s1.reindex(s2.index), 'seq':s2})
df1

! cat /content/anticancer_ENNAACT.txt

import pandas as pd

with open('/content/non-anticancer_ENNAACT.txt', 'r') as f:
    content = f.readlines()

n = len(content)

label = [content[i].strip() for i in range(0,n,2)]
seq = [content[i].strip() for i in range(1,n,2)]

df2 = pd.DataFrame({'label':label,
                   'sequence':seq})

df2

! cat /content/non-anticancer_ENNAACT.txt

! cd-hit -i /content/anticancer_ENNAACT.txt -o /content/anticancer_cdhit.txt -c 0.99

! cat /content/anticancer_cdhit.txt

! ls -l

! grep ">" /content/anticancer_ENNAACT.txt | wc -l

! grep ">" /content/anticancer_cdhit.txt | wc -l

! cd-hit -i /content/non-anticancer_ENNAACT.txt -o non-anticancer_cdhit.txt -c 0.99

! grep ">" /content/non-anticancer_ENNAACT.txt | wc -l

! grep ">" non-anticancer_cdhit.txt | wc -l

import pandas as pd
from Pfeature.pfeature import aac_wp

def aac(input):
  a = input.rstrip('txt')
  output = a + 'aac.csv'
  df_out = aac_wp(input, output)
  df_in = pd.read_csv(output)
  return df_in

  aac('/content/anticancer_ENNAACT.txt')

aac('/content/anticancer_ENNAACT.txt')

import pandas as pd
from Pfeature.pfeature import aac_wp

def aac(input):
  a = input.rstrip('txt')
  output = a + 'aac.csv'
  df_out = aac_wp(input, output)
  df_in = pd.read_csv(output)
  return df_in

  aac('/content/non-anticancer_ENNAACT.txt')

aac('/content/non-anticancer_ENNAACT.txt')

import pandas as pd
from Pfeature.pfeature import dpc_wp

def dpc(input):
  a = input.rstrip('txt')
  output = a + 'dpc.csv'
  df_out = dpc_wp(input, output, 1)
  df_in = pd.read_csv(output)
  return df_in

feature = dpc('/content/anticancer_ENNAACT.txt')
feature

pos = '/content/anticancer_ENNAACT.txt'
neg = '/content/non-anticancer_ENNAACT.txt'

def feature_calc(po, ne, feature_name):
  # Calculate feature
  po_feature = feature_name(po)
  ne_feature = feature_name(ne)
  # Create class labels
  po_class = pd.Series(['ACP' for i in range(len(po_feature))])
  ne_class = pd.Series(['non-ACP' for i in range(len(ne_feature))])
  # Combine po and ne
  po_ne_class = pd.concat([po_class, ne_class], axis=0)
  po_ne_class.name = 'class'
  po_ne_feature = pd.concat([po_feature, ne_feature], axis=0)
  # Combine feature and class
  df = pd.concat([po_ne_feature, po_ne_class], axis=1)
  return df

feature = feature_calc(pos, neg, aac) # AAC
#feature = feature_calc(pos, neg, dpc) # DPC
feature

X = feature.drop('class', axis=1)
y = feature['class'].copy()

X

y

from sklearn.feature_selection import VarianceThreshold

fs = VarianceThreshold(threshold=0.1)
fs.fit_transform(X)
#X2.shape
X2 = X.loc[:, fs.get_support()]
X2

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state =73, stratify=y)

!pip install lazypredict

import sklearn
estimators = sklearn.utils.all_estimators(type_filter=None)
for name, class_ in estimators:
    if hasattr(class_, 'predict_proba'):
        print(name)

import lazypredict
from lazypredict.Supervised import LazyClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import matthews_corrcoef

X = feature.drop('class', axis=1)
y = feature['class'].copy()

# Data split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state =73, stratify=y)

# Defines and builds the lazyclassifier
clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=matthews_corrcoef)
models_train,predictions_train = clf.fit(X_train, X_train, y_train, y_train)
#models_test,predictions_test = clf.fit(X_train, X_test, y_train, y_test)

models_train

models_test

# Plot of Accuracy
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(5, 10))
sns.set_theme(style="whitegrid")
ax = sns.barplot(y=models_train.index, x="Accuracy", data=models_train)
ax.set(xlim=(0, 1))

# Plot of MCC
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(5, 10))
sns.set_theme(style="whitegrid")
ax = sns.barplot(y=models_train.index, x="matthews_corrcoef", data=models_train)
ax.set(xlim=(0, 1))

"""**Random Forest Model**"""

# Build random forest model

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=500)

rf.fit(X_train, y_train)

y_train_pred = rf.predict(X_train)
y_test_pred = rf.predict(X_test)

y_train_pred

y_test_pred

# Simplest and quickest way to obtain the model performance (Accuracy)
rf.score(X_test,y_test)

# Accuracy
from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_test_pred)

# Matthew Correlation Coefficient
from sklearn.metrics import matthews_corrcoef

matthews_corrcoef(y_test, y_test_pred)

# Confusion matrix
from sklearn.metrics import confusion_matrix

confusion_matrix(y_test, y_test_pred)

# Retrieve feature importance from the RF model
importance = pd.Series(rf.feature_importances_, name = 'Gini')

# Retrieve feature names
feature_names = pd.Series(X2.columns, name = 'Feature')

# Classification report
from sklearn.metrics import classification_report

model_report = classification_report(y_train, y_train_pred, target_names=['positive','negative'])

f = open('model_report.txt','w')
f.writelines(model_report)
f.close()

# ROC curve
import matplotlib.pyplot as plt
from sklearn.metrics import plot_roc_curve

plot_roc_curve(rf, X_test, y_test)
plt.show()

plot_roc_curve(rf, X_train, y_train)
plt.show()

df = pd.concat([feature_names, importance], axis=1, names=['Feature', 'Gini'])
df

# Plot of feature importance
import matplotlib.pyplot as plt
import seaborn as sns

df_sorted = df.sort_values('Gini', ascending=False)[:20] # Sort by Gini in descending order; Showing only the top 20 results

plt.figure(figsize=(5, 10))
sns.set_theme(style="whitegrid")
ax = sns.barplot(x = 'Gini', y = 'Feature', data = df_sorted)
plt.xlabel("Feature Importance")

import pandas as pd

with open('/content/StraPep_peptide.txt', 'r') as f:
    content = f.readlines()

n = len(content)


label = list('[content[i].strip() for i in range(0,n,2)]')
seq = [content[i].strip() for i in range(1,n,2)]
s3= pd.Series(label)
s4 = pd.Series(seq)
df3 = pd.DataFrame({'label':s3.reindex(s4.index), 'seq':s4})
df3

! cat /content/StraPep_peptide.txt

import pandas as pd

from Pfeature.pfeature import aac_wp

def aac(input):
  a = input.rstrip('txt')
  output = a + 'aac.csv'
  df_out = aac_wp(input, output)
  df_in = pd.read_csv(output)
  return df_in

  X4=aac('/content/StraPep_peptide.txt')

X4= aac('/content/StraPep_peptide.txt')
X4

import numpy as np
y_train_pred = rf.predict(X4)

y_train_pred

"""**LightGBM Model**"""

import lightgbm as lgb
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import metrics

#LGBM classifier Model
model = lgb.LGBMClassifier(learning_rate=0.09,max_depth=-5,random_state=42)
model.fit(X_train,y_train,eval_set=[(X_train,y_train)],
          verbose=20,eval_metric='logloss')

y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

y_train_pred

y_test_pred

# Accuracy LGBM
from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_test_pred)

# Matthew Correlation Coefficient
from sklearn.metrics import matthews_corrcoef

matthews_corrcoef(y_test, y_test_pred)

import numpy as np
y_train_pred = model.predict(X4)

#LGBM Classifier
y_train_pred

"""Decision Tree Model"""

#Decision Tree Classifier
# Load libraries
import pandas as pd
from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation

# Create Decision Tree classifer object
clf = DecisionTreeClassifier()

# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)

y_train_pred = clf.predict(X_train)
y_test_pred = clf.predict(X_test)

from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_test_pred)

# Matthew Correlation Coefficient
from sklearn.metrics import matthews_corrcoef

matthews_corrcoef(y_test, y_test_pred)

y_train_pred = clf.predict(X4)

#Decision Tree Classifier
y_train_pred















